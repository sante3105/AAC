{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a2a0b1b-9127-4413-a5e9-0b867c8e3e7c",
   "metadata": {},
   "source": [
    "# Exercise 1 \n",
    "\n",
    "Take the original Python script and find the line where the data is scaled. Comment it and perform again the PCA analysis. Is there any consequence? Replot and analyze. How much variance is capture now by PC1 and PC2? Do you still see the quasars separation in the scores plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d174dec2-7a47-4a75-b012-d9322e9ce41c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PCA\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# --- 1. DATA ACQUISITION ---\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Load the data from the CSV file you downloaded from the SDSS SkyServer.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#file_path = 'SDSS_DR14.csv' # Or whatever you named the downloaded file\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# --- 1. DATA ACQUISITION ---\n",
    "# Load the data from the CSV file you downloaded from the SDSS SkyServer.\n",
    "#file_path = 'SDSS_DR14.csv' # Or whatever you named the downloaded file\n",
    "file_path = 'https://drive.usercontent.google.com/download?id=1ZLnCVim0P1Ktewyhd1VcRNWTDCte-W0O&export=download&authuser=1' # Or whatever you named the downloaded file\n",
    "# url = \"https://drive.google.com/file/d/1ZLnCVim0P1Ktewyhd1VcRNWTDCte-W0O/view?usp=drive_link\"\n",
    "\n",
    "try:\n",
    "    # Use skiprows=1 to ignore the first line (\"#Table1\")\n",
    "    sdss_df = pd.read_csv(file_path, skiprows=1)\n",
    "    print(\"SDSS dataset loaded successfully and parsed correctly!\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: '{file_path}'. Please make sure you have downloaded the data and placed it in the correct directory.\")\n",
    "    sdss_df = pd.DataFrame()\n",
    "\n",
    "# Proceed only if the DataFrame was loaded successfully\n",
    "if not sdss_df.empty:\n",
    "    # --- 2. DATA EXPLORATION AND PREPARATION ---\n",
    "    print(\"\\nDataset Shape:\", sdss_df.shape)\n",
    "    print(\"\\nColumns:\", sdss_df.columns)\n",
    "\n",
    "    # Define our features and the target variable for coloring the plot\n",
    "    features = ['u', 'g', 'r', 'i', 'z']\n",
    "    target = 'class'\n",
    "\n",
    "    # --- Safety Check ---\n",
    "    # Verify that all required columns exist in the DataFrame before proceeding.\n",
    "    if target in sdss_df.columns and all(col in sdss_df.columns for col in features):\n",
    "        \n",
    "        X = sdss_df[features]\n",
    "        y = sdss_df[target]\n",
    "\n",
    "        print(\"\\nNumber of samples for each class:\")\n",
    "        print(y.value_counts())\n",
    "\n",
    "        # Standardize the feature data\n",
    "        #X_scaled = StandardScaler().fit_transform(X)\n",
    "        X_scaled = X\n",
    "        print(\"\\nData has been standardized.\")\n",
    "\n",
    "        # --- 3. APPLYING PCA ---\n",
    "        pca = PCA(n_components=2)\n",
    "        X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "        explained_variance = pca.explained_variance_ratio_\n",
    "        print(f\"\\nVariance explained by PC1: {explained_variance[0]:.2%}\")\n",
    "        print(f\"Variance explained by PC2: {explained_variance[1]:.2%}\")\n",
    "        print(f\"Total variance explained: {np.sum(explained_variance):.2%}\")\n",
    "\n",
    "        # --- 4. VISUALIZATION AND INTERPRETATION ---\n",
    "\n",
    "        # Create a new DataFrame for easier plotting with seaborn\n",
    "        pca_df = pd.DataFrame(data=X_pca, columns=['PC1', 'PC2'])\n",
    "        pca_df['class'] = y.values # Add the class labels for coloring\n",
    "\n",
    "        # --- PLOT 1: SCORES PLOT (COLORED) ---\n",
    "        # Scores mean the data is plotted on the new coordinate system : PC1xPC2\n",
    "        plt.figure(figsize=(12, 9))\n",
    "        sns.scatterplot(\n",
    "            x='PC1', y='PC2',\n",
    "            hue='class',\n",
    "            data=pca_df,\n",
    "            palette='viridis',\n",
    "            alpha=0.7,\n",
    "            s=40 # marker size\n",
    "        )\n",
    "        plt.title('PCA of SDSS Astronomical Objects (based on colors)', fontsize=16)\n",
    "        plt.xlabel(f'Principal Component 1 ({explained_variance[0]:.2%})', fontsize=12)\n",
    "        plt.ylabel(f'Principal Component 2 ({explained_variance[1]:.2%})', fontsize=12)\n",
    "        plt.legend(title='Object Class')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        # --- PLOT 2: LOADINGS PLOT ---\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        # In scikit-learn, pca.components_ are the loadings. We transpose for easier plotting.\n",
    "        loadings = pca.components_.T\n",
    "        \n",
    "        for i, feature in enumerate(features):\n",
    "            plt.arrow(0, 0, loadings[i, 0], loadings[i, 1], head_width=0.05, head_length=0.05, color='red', alpha=0.8)\n",
    "            plt.text(loadings[i, 0]*1.15, loadings[i, 1]*1.15, feature, color='black', ha='center', va='center', fontsize=14)\n",
    "\n",
    "        plt.xlim(-0.8, 0.8)\n",
    "        plt.ylim(-0.8, 0.8)\n",
    "        plt.xlabel(f'PC1 ({explained_variance[0]:.2%})')\n",
    "        plt.ylabel(f'PC2 ({explained_variance[1]:.2%})')\n",
    "        plt.title('Loadings Plot', fontsize=16)\n",
    "        plt.gca().set_aspect('equal', adjustable='box')\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        print(\"\\n--- ERROR ---\")\n",
    "        print(f\"The target column '{target}' or one of the feature columns was not found in the data.\")\n",
    "        print(\"Please ensure your downloaded file contains all necessary columns.\")\n",
    "        print(\"Required columns: 'u', 'g', 'r', 'i', 'z', 'class'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c42ceb0-2e7e-4cf8-a83d-bcfdfad7260d",
   "metadata": {},
   "source": [
    "# Analisis\n",
    "\n",
    "Se encontro una disminucion en el porcentaje de covarianza explicado por PC2 y un aumento de porcentaje de covarianza explicado por la componente PC1, sin embargo ¿Esto que significa ?\n",
    "Buscando en \"https://stats.stackexchange.com/questions/22329/how-does-centering-the-data-get-rid-of-the-intercept-in-regression-and-pca\" Encontramos una posible explicacion si tus datos no están centrados (es decir, no se les ha restado la media a cada variable), el “centro de masa” del conjunto de puntos no está en el origen. Como consecuencia, el primer componente principal puede atravesar la nube de puntos de una manera que no coincide con la verdadera dirección de mayor variabilidad de los datos.Esto coincide con el hecho de quela matriz $\\frac{1}{n-1}\\sum (x^T \\cdot x) no coincie con la matriz de covarianza si x no esta centrado "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faa9971-63c8-465c-ba84-aad5e71270db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94fef0a-3263-44db-818a-1abe21eea0b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
